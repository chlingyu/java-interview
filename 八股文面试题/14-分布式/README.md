# 14 - 分布式

> 覆盖分布式系统核心知识：CAP/BASE 理论、分布式事务、分布式 ID、一致性哈希等高频面试考点。分布式事务是面试重灾区。

---

## 一、理论基础

### 1. CAP 理论和 BASE 理论？

> ⭐⭐⭐⭐⭐ 必考 | 难度：⭐⭐⭐

**一句话回答**：CAP 说分布式系统最多同时满足"一致性、可用性、分区容错性"中的两个；BASE 是对 CAP 的妥协，追求最终一致性而不是强一致性。

**通俗理解**：

把分布式系统想象成一家连锁餐厅（多个节点）：

- **一致性（C）** = 所有分店的菜单价格必须一模一样。总店改了价，所有分店必须同步改完才能营业
- **可用性（A）** = 不管哪家分店，客人来了都能点菜，不能说"系统升级中，暂停营业"
- **分区容错性（P）** = 分店之间的电话线断了（网络分区），每家店还得继续营业

**回到技术**："电话线断了"就是网络分区，在分布式环境中这是不可避免的，所以 P 必须保证。剩下只能在 C 和 A 之间选一个：要么保证数据一致但部分请求可能失败（CP），要么保证可用但数据可能短暂不一致（AP）。

| 选择 | 含义 | 典型系统 |
|------|------|---------|
| **CP** | 保一致性，牺牲可用性 | ZooKeeper、Redis Cluster、Etcd |
| **AP** | 保可用性，牺牲一致性 | Eureka、Cassandra、DynamoDB |

**BASE 理论**——CAP 的实际妥协方案：

| 概念 | 全称 | 含义 |
|------|------|------|
| **BA** | Basically Available | 基本可用，允许响应时间变长或功能降级 |
| **S** | Soft State | 软状态，允许数据存在中间状态（不同节点数据暂时不一致） |
| **E** | Eventually Consistent | ⚡**最终一致性**，经过一段时间后数据最终达成一致 |

> BASE 的核心思想：不追求强一致性，而是追求⚡**最终一致性**。大部分互联网系统都是 AP + 最终一致性的方案。

**🎤 面试这样答**：
> "CAP 理论说分布式系统最多同时满足一致性、可用性、分区容错性中的两个。由于网络分区不可避免，P 必须保证，所以实际上是在 CP 和 AP 之间选择。ZooKeeper 是 CP，保证数据一致但 Leader 选举期间不可用；Eureka 是 AP，保证可用但数据可能短暂不一致。BASE 理论是对 CAP 的妥协，核心是最终一致性——允许数据短暂不一致，但最终会达成一致，大部分互联网系统都采用这种方案。"

---

## 二、分布式事务

### 2. 分布式事务有哪些解决方案？

> ⭐⭐⭐⭐⭐ 必考 | 难度：⭐⭐⭐⭐

**一句话回答**：常见方案有 2PC、TCC、本地消息表、MQ 事务消息。没有完美方案，根据业务场景在一致性和性能之间权衡。

**通俗理解**：

分布式事务就像组织一场多人聚餐 AA 付款：
- **2PC（两阶段提交）** = 先问每个人"你带钱了吗？"（准备阶段），所有人都说带了，再说"大家一起付"（提交阶段）。有一个人说没带，全部取消
- **TCC** = 每个人先从钱包里拿出 100 块放桌上（Try 冻结），确认没问题后收走（Confirm），出问题就把钱放回钱包（Cancel）
- **本地消息表** = 你先在自己的账本上记一笔"待转账 100 元"，然后慢慢通知对方，对方收到后确认。最终大家的账本都对上 → ⚡**最终一致性**

**回到技术**："问每个人带钱了吗"就是 2PC 的协调者向所有参与者发送 prepare 请求。"从钱包拿出来放桌上"就是 TCC 的 Try 阶段冻结资源。"在账本上记一笔"就是把消息和业务操作放在同一个本地事务中。

**原理详解**：

**① 2PC（两阶段提交）**：

```
阶段一（Prepare）：协调者问所有参与者"能提交吗？"
  参与者执行事务但不提交，锁定资源，回复 Yes/No

阶段二（Commit/Rollback）：
  所有人都 Yes → 协调者发 Commit，大家一起提交
  有人 No     → 协调者发 Rollback，大家一起回滚
```

> 问题：⚡**同步阻塞**（参与者锁定资源等待协调者指令）、**单点故障**（协调者挂了全卡住）。实际生产中很少直接用原生 2PC。

**② TCC（Try-Confirm-Cancel）**：

| 阶段 | 动作 | 示例（转账） |
|------|------|-------------|
| **Try** | 预留/冻结资源 | 冻结 A 账户 100 元 |
| **Confirm** | 确认执行 | 扣减 A 的冻结金额，B 账户加 100 |
| **Cancel** | 取消，释放资源 | 解冻 A 账户 100 元 |

> 优点：不锁数据库资源，性能好。缺点：⚡**业务侵入性强**，每个操作都要写 Try/Confirm/Cancel 三个方法。

**③ 本地消息表**（最常用）：

```
① A 服务：在同一个本地事务中，执行业务操作 + 往消息表插一条记录
② 定时任务扫描消息表，把未发送的消息发到 MQ
③ B 服务消费消息，执行业务操作，完成后通知 A 更新消息状态
④ 如果 B 处理失败，MQ 会重试投递 → 保证最终一致性
```

> 这是⚡**最实用的方案**，实现简单，保证最终一致性，适合大部分业务场景。

**④ Seata 框架**：阿里开源的分布式事务框架，支持 AT（自动补偿）、TCC、Saga 等模式，⚡**AT 模式**对业务无侵入，是目前 Java 生态最流行的分布式事务方案。

**🎤 面试这样答**：
> "分布式事务常见方案有四种：2PC 两阶段提交，强一致但有同步阻塞和单点故障问题；TCC 通过 Try/Confirm/Cancel 三个阶段实现，性能好但业务侵入性强；本地消息表是最实用的方案，把消息和业务放在同一个本地事务中，通过定时任务和 MQ 保证最终一致性；还有 Seata 框架的 AT 模式，对业务无侵入，自动生成回滚日志。实际项目中，如果追求最终一致性，本地消息表最简单；如果需要框架支持，用 Seata。"

---

## 三、分布式 ID

### 3. 分布式 ID 怎么生成？

> ⭐⭐⭐⭐ 高频 | 难度：⭐⭐⭐

**一句话回答**：常见方案有 UUID、数据库自增、号段模式、雪花算法（Snowflake）。生产环境最常用的是雪花算法。

**通俗理解**：

分布式系统有多台机器同时生成 ID，就像多个窗口同时发号。要保证每个窗口发出的号不重复，有几种思路：直接用随机数（UUID）、找一个中心发号器（数据库自增）、或者每个窗口按规则自己编号（雪花算法）。

**回到技术**："随机数"就是 UUID，本地生成不依赖外部，但无序且太长不适合做数据库主键。"中心发号器"就是数据库自增 ID，简单有序但有单点瓶颈。"按规则自己编号"就是雪花算法，用时间戳+机器ID+序列号组成 64 位 ID，既有序又高性能。

| 方案 | 原理 | 优点 | 缺点 |
|------|------|------|------|
| **UUID** | 随机生成 128 位字符串 | 简单，本地生成 | 无序，不适合做主键（B+树频繁分裂）；太长 |
| **数据库自增** | 用一张表的 auto_increment | 简单，有序 | 性能瓶颈，单点故障 |
| **号段模式** | 每次从数据库取一批 ID 缓存在本地 | 减少数据库访问 | 实现稍复杂 |
| **雪花算法** | ⚡**64 位 = 时间戳 + 机器 ID + 序列号** | 有序、高性能、不依赖外部 | 依赖机器时钟，时钟回拨会重复 |

**雪花算法结构**：

```
0 | 0000000000 0000000000 0000000000 0000000000 0 | 00000 00000 | 000000000000
符号位  41 位时间戳（毫秒，可用约 69 年）          10 位机器 ID    12 位序列号
                                                 (最多 1024 台)  (每毫秒 4096 个)
```

> **生产环境推荐**：⚡**雪花算法**或基于它的变种（如百度 UidGenerator、美团 Leaf）。既能保证全局唯一，又是趋势递增的，适合做数据库主键。

**🎤 面试这样答**：
> "分布式 ID 常见方案有 UUID、数据库自增、号段模式和雪花算法。生产环境最常用雪花算法，它用 64 位组成：41 位时间戳、10 位机器 ID、12 位序列号，每毫秒每台机器能生成 4096 个 ID，既全局唯一又趋势递增。缺点是依赖机器时钟，时钟回拨会导致 ID 重复，可以用美团 Leaf 等框架解决。"

---

## 四、一致性哈希

### 4. 什么是一致性哈希？

> ⭐⭐⭐ 常问 | 难度：⭐⭐⭐

**回答**：一致性哈希是一种分布式哈希算法，解决的是"节点增减时大量数据迁移"的问题。你可以理解为把所有节点和数据都映射到一个环上，数据顺时针找到最近的节点存放。

**普通哈希的问题**：`hash(key) % N`，当节点数 N 变化时（扩容/缩容），几乎所有 key 的映射都会变，导致⚡**大量缓存失效**。

**一致性哈希的做法**：

```
① 把哈希值空间组织成一个环（0 ~ 2³²-1）
② 节点和数据都映射到环上
③ 数据顺时针找到最近的节点存放
④ 增减节点时，只影响相邻节点的数据 → 迁移量极小
```

> **数据倾斜问题**：如果节点太少，可能大部分数据都落在某一个节点上。解决办法是引入⚡**虚拟节点**——每个物理节点映射多个虚拟节点到环上，让数据分布更均匀。

---

## 五、分布式锁

### 5. 分布式锁怎么实现？Redis 和 ZooKeeper 的对比？

> ⭐⭐⭐⭐ 高频 | 难度：⭐⭐⭐

**一句话回答**：分布式锁用于多节点环境下保证同一时刻只有一个节点执行某段逻辑。主流方案是 Redis（SET NX + Lua）和 ZooKeeper（临时有序节点），Redis 性能高但可靠性稍弱，ZooKeeper 可靠性强但性能稍低。

**通俗理解**：

多台服务器抢同一个资源，就像多个人抢一间会议室：

- **Redis 分布式锁** = 在会议室门口贴一张便利贴"张三占用中"。简单快速，但如果张三突然晕倒（宕机），便利贴还贴着，别人进不去。虽然可以设过期时间（便利贴自动掉落），但可能张三还没开完会便利贴就掉了（锁提前过期）
- **ZooKeeper 分布式锁** = 在前台登记排队，前台实时监控每个人的状态。张三晕倒了，前台立刻知道并通知下一个人进去。更可靠，但每次都要去前台登记，比贴便利贴慢

**回到技术**："贴便利贴"就是 Redis 的 `SET key value NX EX` 命令，NX 保证只有一个客户端能设置成功，EX 设置过期时间防止死锁。"前台登记"就是 ZooKeeper 创建临时有序节点，客户端断开连接后临时节点自动删除，天然避免死锁。

| 对比项 | Redis | ZooKeeper |
|--------|-------|-----------|
| 实现方式 | `SET key NX EX` + Lua 脚本释放 | 临时有序节点 + Watch 监听 |
| 性能 | ⚡**高**（内存操作，QPS 十万级） | 较低（需要写磁盘、节点同步） |
| 可靠性 | 主从切换可能丢锁（异步复制） | ⚡**强**（ZAB 协议保证一致性） |
| 锁过期问题 | 需要设过期时间，可能提前过期 | ⚡**无此问题**（临时节点随会话失效） |
| 可重入 | 需要自己实现（记录线程标识+计数） | Curator 框架原生支持 |
| 适用场景 | 高并发、对性能要求高、允许极端情况下短暂不一致 | 对一致性要求高的场景 |

```java
// Redis 分布式锁（Redisson 框架，生产推荐）
RLock lock = redisson.getLock("order:lock:" + orderId);
try {
    // 尝试加锁，最多等 3 秒，锁自动过期 10 秒
    // Redisson 内置看门狗机制，自动续期，解决锁提前过期问题
    if (lock.tryLock(3, 10, TimeUnit.SECONDS)) {
        // 执行业务逻辑
    }
} finally {
    lock.unlock(); // 释放锁
}
```

> **生产建议**：用 Redis 分布式锁就用 ⚡**Redisson** 框架，不要自己手写。Redisson 内置了看门狗（Watchdog）自动续期、可重入锁、红锁（RedLock）等能力，解决了手写锁的各种坑。

**🎤 面试这样答**：
> "分布式锁主流有 Redis 和 ZooKeeper 两种方案。Redis 用 SET NX EX 加锁，性能高，但主从切换时可能丢锁，而且需要处理锁过期问题，生产中推荐用 Redisson 框架，它内置看门狗自动续期。ZooKeeper 用临时有序节点实现，客户端断开连接锁自动释放，可靠性更强，但性能不如 Redis。一般高并发场景选 Redis，对一致性要求高的场景选 ZooKeeper。"

---

## 面试高频程度排序（3~5 年）

| 优先级 | 题目 |
|--------|------|
| ⭐⭐⭐⭐⭐ | CAP 理论和 BASE 理论？ |
| ⭐⭐⭐⭐⭐ | 分布式事务有哪些方案？ |
| ⭐⭐⭐⭐ | 分布式 ID 怎么生成？ |
| ⭐⭐⭐⭐ | 分布式锁（Redis vs ZooKeeper）？ |
| ⭐⭐⭐ | 一致性哈希算法？ |